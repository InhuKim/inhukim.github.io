---
layout: post
title:  "타이타닉 생존자 예측 -kaggle 데이터셋-"
summary: 캐글의 타이타닉 생존자 데이터셋을 분석하고 간단한 머신러닝 모델을 적용.
author: 김인후
date: '2022-03-16'
category: Machine_Learning
thumbnail: /assets/img/posts/kaggle-titanic-challenge.jpg
keywords: 머신러닝, 타이타닉, titanic, prediction
usemathjax: true
permalink: /blog/Titanic-machinelearning/
---


# 1. 데이터 분석

 - pandas 라이브러리와 seaborn 라이브러리를 사용해서 데이터들을 데이터프레임에 넣고 해당 데이터를 seaborn을 통해서 확인
 - 결측치와 예측에 사용 되지 못할 문자열 데이터 column을 삭제.
 - heatmap을 통해서 각 데이터끼리의 상관관계를 확인.

<br>
<hr>

#### 라이브러리 및 데이터셋


```python
import pandas as pd
import seaborn as sns
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
```


```python
titanic_dataset = pd.read_csv('train.csv')
titanic_test = pd.read_csv('test.csv')

titanic_dataset.head()
```




<div>
<table class="dataframe text-white">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody style="text-align: center">
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>


<br>
<hr>
<br><br>


### EDA(Exploratory Data Analysis)
 - 학습 데이터는 모두 891개의 데이터를 가지고 있다.
 - 생존 여부, 객실 등급, 이름, 성별, 나이, 동승 가족들, 티켓 번호, 요금, 탑승한 항구의 11개 데이터
 - 성별과 객실 번호에는 많은 결측치가 존재하고 탑승 항구도 2개의 결측치가 존재.


```python
# 타이타닉 데이터셋이 가지고 있는 데이터 내용
titanic_dataset.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  891 non-null    int64  
     1   Survived     891 non-null    int64  
     2   Pclass       891 non-null    int64  
     3   Name         891 non-null    object 
     4   Sex          891 non-null    object 
     5   Age          714 non-null    float64
     6   SibSp        891 non-null    int64  
     7   Parch        891 non-null    int64  
     8   Ticket       891 non-null    object 
     9   Fare         891 non-null    float64
     10  Cabin        204 non-null    object 
     11  Embarked     889 non-null    object 
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB

<br><br>
<hr>
<br>

### 생존자 그래프


```python

sns.countplot(x='Survived', data=titanic_dataset)
live = titanic_dataset.Survived.value_counts()[0]
print('survive: ' + str(live))
```

    survive: 549
    
<br>

    
<img src="/assets/img/posts/Titanic_7_1.png" class="bg-light" alt="">


<br>
<hr>
<br>


### 성별에 따른 생존자


```python
gender_labels = ["Male", "Female"]
gender_survived = [len(titanic_dataset[(titanic_dataset.Survived == 1) & (titanic_dataset.Sex == 'male')]), len(titanic_dataset[(titanic_dataset.Survived == 1) & (titanic_dataset.Sex == 'female')])]
sns.barplot(gender_labels, gender_survived)
```

    C:\Users\marka\anaconda3\lib\site-packages\seaborn\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
      warnings.warn(
    




    <AxesSubplot:>




    
<img src="/assets/img/posts/Titanic_9_2.png" class="bg-light" alt="">


<br>
<hr>
<br>



### 객실의 등급에 따른 생존자 숫자


```python
sns.countplot(x='Survived', hue='Pclass', data=titanic_dataset)
```




    <AxesSubplot:xlabel='Survived', ylabel='count'>




    
<img src="/assets/img/posts/Titanic_11_1.png" class="bg-light" alt="">



<br>
<hr>
<br>


### 가족 동승자 관련 생존자 숫자

 - 동승한 가족의 숫자를 나타내기 때문에 해당 수치의 크기는 생존의 여부와 큰 관계를 보이기 힘들다.
 - 가족의 숫자의 총합의 평균을 나타낸 것인데 큰 연관성이 보이지 않는다.


```python
# 가족 동승자 숫자의
titanic_dataset[['Survived', 'SibSp', 'Parch']].groupby('Survived').mean()
```




<div>
<table class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>SibSp</th>
      <th>Parch</th>
    </tr>
    <tr>
      <th>Survived</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody style="text-align: center">
    <tr>
      <th>0</th>
      <td>0.553734</td>
      <td>0.329690</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.473684</td>
      <td>0.464912</td>
    </tr>
  </tbody>
</table>
</div>



<hr>
<br>

### 문자열 데이터 값 변환


```python
# embarked 데이터 값 변환
# Cherbourg = 1, Queenstown = 2, Southampton = 3

corr = titanic_dataset.drop(['PassengerId'], axis=1)

corr['Sex'] = corr['Sex'].map({'male':0, 'female':1})
corr['Embarked'] = corr['Embarked'].map({'C': 1, 'Q': 2, 'S': 3})
```


```python
# 문자열 데이터 삭제
# 데이터프레임에서 완전히 삭제하는 것이기 때문에 *해당 셀은 한 번만 실행*하고 다시 실행하지 않음.
corr.drop(["Name"], axis=1, inplace=True)
corr.drop(['Ticket'], axis=1, inplace=True)
corr.drop(['Cabin'], axis=1, inplace=True)
```

<br>
<hr>


### 상관관계도


```python
# 상관관계를 히트맵으로 표현.
heat_data = corr.corr()
sns.heatmap(heat_data)
```




    <AxesSubplot:>




    
<img src="/assets/img/posts/Titanic_17_1.png" class="bg-light" alt="">



<br>



```python
corr.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 8 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   Survived  891 non-null    int64  
     1   Pclass    891 non-null    int64  
     2   Sex       891 non-null    int64  
     3   Age       714 non-null    float64
     4   SibSp     891 non-null    int64  
     5   Parch     891 non-null    int64  
     6   Fare      891 non-null    float64
     7   Embarked  889 non-null    float64
    dtypes: float64(3), int64(5)
    memory usage: 55.8 KB


<br>
<hr>
<br>


```python
# 나이 결측치에 객실 등급별 나이 평균을 입력.
# 남은 데이터 중에서 성별은 두가지 경우로 평균값이 너무 획일되었고, 나머지는 평균값을 구하기 어려운 데이터이기 떄문에.

def add_age(cols):
    age = cols[0]
    pclass = cols[1]

    if pd.isnull(age):
        if pclass == 1:
            return titanic_dataset[titanic_dataset['Pclass'] == 1]['Age'].mean()
        elif pclass == 2:
            return titanic_dataset[titanic_dataset['Pclass'] == 2]['Age'].mean()
        elif pclass == 3:
            return titanic_dataset[titanic_dataset['Pclass'] == 3]['Age'].mean()
    else:
        return age

corr['Age'] = corr[['Age', 'Pclass']].apply(add_age, axis=1)
```


```python
corr['Embarked'] = corr['Embarked'].fillna(value=1)
corr.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 8 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   Survived  891 non-null    int64  
     1   Pclass    891 non-null    int64  
     2   Sex       891 non-null    int64  
     3   Age       891 non-null    float64
     4   SibSp     891 non-null    int64  
     5   Parch     891 non-null    int64  
     6   Fare      891 non-null    float64
     7   Embarked  891 non-null    float64
    dtypes: float64(3), int64(5)
    memory usage: 55.8 KB
    

##### 결측치가 모두 제거되었고 문자열 없이 숫자 데이터로 정제가 완료되었다. 예측에 불필요할 것으로 보이는 데이터들 또한 제거하였다.



<hr>
<br>


# 2. Marchine Learning Model

 - 사용한 모델은 사이킷런의 로지스틱 회귀와 SVM을 사용.
 - 데이터 분할은 따로 처리하지 않았으며 기존에 배포된 데이터셋에서 이미 train, test로 나뉘어 있기 때문에 정형한 데이터를 데이터프레임으로 만들어 직접 분리.


```python
# 학습 데이터 : x_train은 모델에 학습을 진행하는 데이터이고 y_train은 학습시키는 데이터의 정답이다.
y_train = corr['Survived']
x_train = corr.drop(['Survived'], axis=1)
```


```python
print(y_train)
```

    0      0
    1      1
    2      1
    3      1
    4      0
          ..
    886    0
    887    1
    888    0
    889    1
    890    0
    Name: Survived, Length: 891, dtype: int64
    


```python
# x_test_data는 학습 데이터와 완전히 분리된 데이터로 학습된 모델에 입력하는 데이터로 해당 데이터를 통해서 예측값을 생성한다.
x_test_data = titanic_test.drop(['PassengerId'], axis=1)
x_test_data
```




<div>
<table class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody style="text-align: center">
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Kelly, Mr. James</td>
      <td>male</td>
      <td>34.5</td>
      <td>0</td>
      <td>0</td>
      <td>330911</td>
      <td>7.8292</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>Wilkes, Mrs. James (Ellen Needs)</td>
      <td>female</td>
      <td>47.0</td>
      <td>1</td>
      <td>0</td>
      <td>363272</td>
      <td>7.0000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>Myles, Mr. Thomas Francis</td>
      <td>male</td>
      <td>62.0</td>
      <td>0</td>
      <td>0</td>
      <td>240276</td>
      <td>9.6875</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>Wirz, Mr. Albert</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>315154</td>
      <td>8.6625</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>
      <td>female</td>
      <td>22.0</td>
      <td>1</td>
      <td>1</td>
      <td>3101298</td>
      <td>12.2875</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>413</th>
      <td>3</td>
      <td>Spector, Mr. Woolf</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>A.5. 3236</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>414</th>
      <td>1</td>
      <td>Oliva y Ocana, Dona. Fermina</td>
      <td>female</td>
      <td>39.0</td>
      <td>0</td>
      <td>0</td>
      <td>PC 17758</td>
      <td>108.9000</td>
      <td>C105</td>
      <td>C</td>
    </tr>
    <tr>
      <th>415</th>
      <td>3</td>
      <td>Saether, Mr. Simon Sivertsen</td>
      <td>male</td>
      <td>38.5</td>
      <td>0</td>
      <td>0</td>
      <td>SOTON/O.Q. 3101262</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>416</th>
      <td>3</td>
      <td>Ware, Mr. Frederick</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>359309</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>417</th>
      <td>3</td>
      <td>Peter, Master. Michael J</td>
      <td>male</td>
      <td>NaN</td>
      <td>1</td>
      <td>1</td>
      <td>2668</td>
      <td>22.3583</td>
      <td>NaN</td>
      <td>C</td>
    </tr>
  </tbody>
</table>
<p>418 rows × 10 columns</p>
</div>


<br>
<hr>



### test dataset 가공


```python
# 해당 셀도 한 번만 실행.
x_test_data['Sex'] = x_test_data['Sex'].map({'male':0, 'female':1})
x_test_data['Embarked'] = x_test_data['Embarked'].map({'C': 1, 'Q': 2, 'S': 3})

x_test_data.drop(["Name"], axis=1, inplace=True)
x_test_data.drop(['Ticket'], axis=1, inplace=True)
x_test_data.drop(['Cabin'], axis=1, inplace=True)

x_test_data['Age'] = x_test_data[['Age', 'Pclass']].apply(add_age, axis=1)

x_test_data['Embarked'] = x_test_data['Embarked'].fillna(value=1)
x_test_data['Fare'] = x_test_data['Fare'].fillna(value=x_test_data['Fare'].mean())
```


```python
x_test_data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 418 entries, 0 to 417
    Data columns (total 7 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   Pclass    418 non-null    int64  
     1   Sex       418 non-null    int64  
     2   Age       418 non-null    float64
     3   SibSp     418 non-null    int64  
     4   Parch     418 non-null    int64  
     5   Fare      418 non-null    float64
     6   Embarked  418 non-null    int64  
    dtypes: float64(2), int64(5)
    memory usage: 23.0 KB


<br>
<hr>
<br>

### 로지스틱 회귀 모델 사용


```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(x_train, y_train)
predictions = model.predict(x_test_data)
```


```python
y_test_data = pd.read_csv('gender_submission.csv')
y_test_data = y_test_data.drop(['PassengerId'], axis=1)
```


```python
# 리더보드에서 1.0 정답값을 가져왔음.

y_test_data = pd.read_csv('submission.csv')
y_test_data = y_test_data.drop(['PassengerId'], axis=1)

print(classification_report(y_test_data, predictions))
print(accuracy_score(y_test_data, predictions))
```

```
                 precision    recall  f1-score   support

           0       0.81      0.82      0.82       260
           1       0.70      0.69      0.69       158

    accuracy                           0.77       418
   macro avg       0.76      0.75      0.76       418
weighted avg       0.77      0.77      0.77       418

0.7703349282296651

```



<br>
<hr>
<br>


### SVM(Support Vector Machine) 모델 사용


```python
from sklearn import svm

model2 = svm.SVC(kernel='linear')
model2.fit(x_train, y_train)
predictions2 = model2.predict(x_test_data)
```


```python
accuracy_score(y_test_data, predictions2)
```


    0.7655502392344498


<p> Accuracy가 100%가 나왔기 때문에 혹시 학습이나 테스트 과정에서 데이터가 겹쳐 과적합이 이루어진 것인지 아니면 테스트를 잘못 돌린 것인지 
확인하였지만, 계속 동일하게 1.0 결과가 나타났다. </p>
<p> 나중에 스터디에서 서로 코드리뷰를 진행하면서 kaggle에서 제공한 정답 데이터가 예시일 뿐이고 실제로는 리더보드에 넣어 봐야지 옳바른 정답
 데이터를 얻을 수 있다는 것을 배웠다. 일단 1.0을 찍은 코드의 결과값을 가져와서 임시로 돌려봤을 때, 다시 76%정도의 결과를 얻을 수 있었다.</p>


<br>
<hr>
<br>

# 3. 결론

 - 예제로 돌아다니는 잘 정리된 데이터로 학습을 돌렸기 때문에 기본적으로 높은 정확도를 보여주고 있다.
 - 테스트 데이터를 잘 알아봐야 하고 SVM은 rbf 커널을 사용했을 경우에는 연산이 끝나지 않을 정도로 오래 걸렸다.

<br>
<hr>

### Reference

 - https://www.kaggle.com/c/titanic
 - https://dacon.io/competitions/open/235539/codeshare/1534?page=1&dtype=recent
 - https://www.kaggle.com/code/anjusunilkumar/titanic-dataset-prediction/notebook
